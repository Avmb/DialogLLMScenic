#! /usr/bin/env python3

import sys
import os
import argparse
import json
import re
from langchain.llms import OpenAI
from langchain.prompts.prompt import PromptTemplate
from langchain.prompts.few_shot import FewShotPromptTemplate
from langchain.prompts.example_selector.base import BaseExampleSelector
import numpy as np
import backoff  # for exponential backoff
import openai  # for OpenAI API calls

# Edited from paragraph of Scenic documentation: https://scenic-lang.readthedocs.io/en/latest/index.html
prompt_prefix = """# Scenic is a domain-specific probabilistic programming language for modeling the environments of cyber-physical systems like robots and autonomous cars. A Scenic program defines a distribution over scenes, configurations of physical objects and agents; sampling from this distribution yields concrete scenes which can be simulated to produce training or testing data. Scenic can also define (probabilistic) policies for dynamic agents, allowing modeling scenarios where agents take actions over time in response to the state of the world.

# The following is a list of Scenic scenarios, each with its corresponding description in English:
"""

from transformers import GPT2TokenizerFast # for token count
gpt2_tokenizer = GPT2TokenizerFast.from_pretrained("gpt2", model_max_length=4096)
def get_num_tokens(text):
    return len(gpt2_tokenizer(text)['input_ids'])

def get_example_num_tokens(example):
    example_text = "# Scenario: {body}\n\n# Description: {docstring}\n\n###\n\n".format(body=example['body'],
                                                                                        docstring=example['docstring'])
    return get_num_tokens(example_text)

def escape_example_str(example_str):
    example_str = example_str.replace("<", "<q").replace(">", "q>")
    example_str = example_str.replace("{", "<_").replace("}", "_>")
    return example_str

def unescape_example_str(example_str):
    example_str = example_str.replace("<_", "{").replace("_>", "}")
    example_str = example_str.replace("<q", "<").replace("q>", ">")
    return example_str

def texualize_example_dict(example):
    rv = dict()
    for k, v in example.items():
        k_str = escape_example_str(str(k))
        v_str = escape_example_str(str(v))
        rv[k_str] = v_str
    return rv

class RandomExampleSelector(BaseExampleSelector):
    def __init__(self, examples, max_prompt_size, prefix_length=0):
        self.examples = examples
        self.max_prompt_size = max_prompt_size
        self.prefix_length = prefix_length

    def add_example(self, example):
        """Add new example to store for a key."""
        self.examples.append(example)

    def select_examples(self, input_variables):
        """Select which examples to use based on the inputs."""
        shuffled_examples = np.random.permutation(self.examples)
        remaining_length = self.max_prompt_size - self.prefix_length
        rv = []
        for example in shuffled_examples:
            if remaining_length < example["num_tokens"]:
                break
            remaining_length -= example["num_tokens"]
            rv.append({"body": example["body"], "docstring": example["docstring"]})
        return rv


@backoff.on_exception(backoff.expo, openai.error.RateLimitError)
def generate_candidate_augmentations(lm, prompt_text):
    lm_params = lm._default_params.copy()
    lm_params["stop"]="\n\n###"
    lm_response = lm.client.create(model=lm.model_name, prompt=prompt_text, **lm_params)
    candidate_augmentations = [x["text"].strip() for x in lm_response["choices"]]
    return candidate_augmentations


def main():
    parser = argparse.ArgumentParser(description = 'Augment Scenic scenarios with alternate questions generated by GPT-3')
    parser.add_argument("-i", "--in-file", type=str, required=True,
                        help="JSON input file with original Scenic scenarios with docstrings")
    parser.add_argument("-g", "--generated-file", type=str, required=True,
                        help="JSON input file with the generated Scenic scenarios with docstrings")
    parser.add_argument("-o", "--out-file", type=str, required=True,
                        help="JSON output file where the scenarios with additional question augmentations are to be saved")
    parser.add_argument("-n", "--num-augmentations", type=int, required=True,
                        help="maximum number of augmentations per example to generate")
    parser.add_argument("-m", "--model-name", type=str, default="text-davinci-003",
                        help="OpenAI language model name to use. Default: %(default)s")
    parser.add_argument("-t", "--model-temp", type=float, default=0.7,
                        help="Generation temperature. Default: %(default)s")
    parser.add_argument("-k", "--api-key-file", type=str, default="",
                        help="File name of the OpenAI API key to use. Default: use the key in the environment variable OPENAI_API_KEY")
    parser.add_argument("-p", "--max-prompt-size", type=int, default=3096,
                        help="Maximum prompt size in tokens. Default: %(default)s")
    args = parser.parse_args()

    if args.api_key_file == "":
        if "OPENAI_API_KEY" not in os.environ:
            print("Error: no OpenAI API key.", file=sys.stderr)
            sys.exit(-1)
    else:
        with open(args.api_key_file, "r") as api_key_fs:
            os.environ["OPENAI_API_KEY"] = api_key_fs.read().strip()

    print("Loading original examples...", end="", file=sys.stderr)
    with open(args.in_file, "r") as in_fs:
        orig_scenarios = json.load(in_fs)
    str_orig_scenarios = [texualize_example_dict(e) for e in orig_scenarios]
    for example in str_orig_scenarios:
        example["num_tokens"] = get_example_num_tokens(example)
    print(" done.", file=sys.stderr)
    print("Loading generated examples...", end="", file=sys.stderr)
    with open(args.generated_file, "r") as in_fs:
        scenarios = json.load(in_fs)
    str_scenarios = [texualize_example_dict(e) for e in scenarios]
    for example in str_scenarios:
        example["num_tokens"] = get_example_num_tokens(example)
    print(" done.", file=sys.stderr)

    print("Generation...", end="", file=sys.stderr, flush=True)
    lm = OpenAI(model_name=args.model_name, temperature=args.model_temp, n=args.num_augmentations, best_of=args.num_augmentations)
    example_prompt = PromptTemplate(input_variables=["docstring", "body"], template="# Scenario: {body}\n\n# Description: {docstring}\n\n###")

    for i in range(len(str_scenarios)):
        cur_example = str_scenarios[i]
        #other_examples = str_scenarios[:i] + str_scenarios[i+1:]
        other_examples = str_orig_scenarios
        example_selector = RandomExampleSelector(other_examples, args.max_prompt_size, prefix_length=get_num_tokens(prompt_prefix))
        prompt = FewShotPromptTemplate(
                example_selector=example_selector,
                example_prompt=example_prompt,
                prefix=prompt_prefix,
                suffix="# Scenario: {input_body}\n\n# Description:",
                input_variables=["input_body"])
        prompt_text = prompt.format(input_body = cur_example['body'])
        prompt_text = unescape_example_str(prompt_text)
        # call GPT-3 directly in order to generate multiple continuations
        candidate_augmentations = generate_candidate_augmentations(lm, prompt_text)
        # extract unique augmentations
        original_d = unescape_example_str(cur_example['docstring'])
        description_set = set([original_d])
        for d in candidate_augmentations:
            description_set.add(d)
        description_set.remove(original_d)
        augmentations = sorted(list(description_set))
        # save in original example dict
        scenarios[i]["augmented_docstrings"] = augmentations

        if (i % 10) == 0:
            print(" %i..." % (i+1), end="", file=sys.stderr, flush=True)

    if (i % 10) != 0:
        print(" %i." % (i+1), end="", file=sys.stderr)
    print(" Done.\nSaving...", end="", file=sys.stderr, flush=True)
    with open(args.out_file, "w") as out_fs:
        json.dump(scenarios, out_fs, indent=1, sort_keys=True)
    print(" Done.", file=sys.stderr)

if __name__ == "__main__":
    main()

